
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- This HTML was auto-generated from MATLAB code. To make changes, update the MATLAB code and republish this document.       --><title>Optimization Toolbox™ ソルバーによる記号数学の使用</title><meta name="generator" content="MATLAB 7.11"><link rel="schema.DC" href="../http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2010-05-27"><meta name="DC.source" content="symbolic_optim_demo.m"><link rel="stylesheet" type="text/css" href="../../../../matlab/demos/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit symbolic_optim_demo">エディターで symbolic_optim_demo.m を開く</a></div><div class="right"><a href="matlab:echodemo symbolic_optim_demo">コマンド ウィンドウで実行</a></div></div><div class="content"><h1>Optimization Toolbox™ ソルバーによる記号数学の使用</h1><!--introduction--><p>一般に、Optimization Toolbox™ ソルバーの精度と効率は、目的関数と制約関数の勾配とヘッセ行列が与えられた場合に向上します。このデモでは、<tt>jacobian</tt> と <tt>matlabFunction</tt> という Symbolic Math Toolbox™ の関数を使用して、これらの導関数を最適化ソルバーに与える方法を示します。</p><p><b>追加要件:</b></p><div><ul><li>Symbolic Math Toolbox</li></ul></div><p>最適化関数でシンボリックな計算を使用する際は、以下のことを考慮しなければなりません。</p><p>1. 最適化の目的関数と制約関数は、<tt>x</tt> のように、ベクトルで定義する必要があります。ただし、シンボリック変数はベクトル値ではなく、スカラーまたは複素数値です。このため、ベクトルとスカラー間の変換が必要です。</p><p>2. 最適化の勾配と、場合によりますがヘッセ行列は、目的関数または制約関数の本体内で計算されることになっています。これは、シンボリックな勾配またはヘッセ行列を、目的関数または制約関数ファイル、あるいは関数ハンドル内の適切な位置に配置する必要があることを意味します。</p><p>3. 勾配とヘッセ行列のシンボリックな計算には、時間がかかることがあります。このため、ソルバーの実行中に呼び出すために、この計算を一度だけ実行し、<tt>matlabFunction</tt> を使用してコードを生成する必要があります。</p><p>4. <tt>subs</tt> 関数を使用したシンボリック式の評価には、時間がかかります。<tt>matlabFunction</tt> を使用した方が、はるかに効率的です。</p><p>5. <tt>matlabFunction</tt> は、入力ベクトルの方向に依存するコードを生成します。<tt>fmincon</tt> は列ベクトルを含む目的関数を呼び出すため、シンボリック変数の列ベクトルを含む <tt>matlabFunction</tt> を呼び出す際は注意が必要です。</p><!--/introduction--><h2>目次</h2><div><ul><li><a href="#1">第 1 の例: ヘッセ行列を使用した制約なし最小化</a></li><li><a href="#7">第 2 の例: fmincon の内点法アルゴリズムを使用した制約付き最小化</a></li><li><a href="#14">シンボリック変数のクリーンアップ</a></li></ul></div><h2>第 1 の例: ヘッセ行列を使用した制約なし最小化<a name="1"></a></h2><p>最小化する目的関数は以下のとおりです。</p><p><img src="../symbolic_optim_demo_eq87848.png" alt="$$f(x_1, x_2) = \log\left (1 + 3 \left ( x_2 - (x_1^3 - x_1)
\right )^2 + (x_1 - 4/3)^2 \right ).$$"></p><p>この関数は正であり、<tt>x1</tt> = 4/3, <tt>x2</tt> =(4/3)^3 - 4/3 = 1.0370... において固有の最小値ゼロに到達します。</p><p>独立変数を <tt>x1</tt> および <tt>x2</tt> として記述します。その理由は、この形式で、独立変数をシンボリック変数として使用できるからです。ベクトル <tt>x</tt> の要素としては、<tt>x(1)</tt> と <tt>x(2)</tt> と記述されます。この関数は、次のプロットで示したように、曲がりくねった谷を持っています。</p><pre class="codeinput">syms <span class="string">x1</span> <span class="string">x2</span> <span class="string">real</span>
x = [x1;x2]; <span class="comment">% column vector of symbolic variables</span>
f = log(1 + 3*(x2 - (x1^3 - x1))^2 + (x1 - 4/3)^2);

ezsurfc(f,[-2 2])
view(127,38)
</pre><img vspace="5" hspace="5" src="../symbolic_optim_demo_01.png" alt=""> <p>f の勾配とヘッセ行列を計算します。</p><pre class="codeinput">gradf = jacobian(f,x).'; <span class="comment">% column gradf</span>
hessf = jacobian(gradf,x);
</pre><p><tt>fminunc</tt> ソルバーには、ベクトル x を渡し、<tt>GradObj</tt> オプションと <tt>Hessian</tt> オプションを [on] に設定し、3 つの出力のリスト [f(x),gradf(x),hessf(x)] を与えます。</p><p><tt>matlabFunction</tt> は、3 つの入力のリストから 3 つの出力のこのリストを正確に生成します。<tt>matlabFunction</tt> はさらに、<tt>vars</tt> オプションを使用してベクトル入力を受け取ります。</p><pre class="codeinput">fh = matlabFunction(f,gradf,hessf,<span class="string">'vars'</span>,{x});
</pre><p>点 [-1,2] から始めて、最小化問題を解きます。</p><pre class="codeinput">options = optimset(<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="string">'Hessian'</span>,<span class="string">'on'</span>, <span class="keyword">...</span>
    <span class="string">'Display'</span>,<span class="string">'final'</span>);
[xfinal fval exitflag output] = fminunc(fh,[-1;2],options)
</pre><pre class="codeoutput">
Local minimum possible.

fminunc stopped because the final change in function value relative to 
its initial value is less than the default value of the function tolerance.




xfinal =

   1.333332064357340
   1.037030117814729


fval =

    7.662315226723625e-012


exitflag =

     3


output = 

         iterations: 14
          funcCount: 15
       cgiterations: 11
      firstorderopt: 3.439061368233648e-005
          algorithm: 'large-scale: trust-region Newton'
            message: [1x472 char]
    constrviolation: []

</pre><p>これを、勾配またはヘッセ行列の情報を使用しない場合の反復の回数と比較します。比較するには、<tt>LargeScale</tt> オプションを <tt>[off]</tt> に設定することで得られる中規模なアルゴリズムが必要です。</p><pre class="codeinput">options = optimset(<span class="string">'Display'</span>,<span class="string">'final'</span>,<span class="string">'LargeScale'</span>,<span class="string">'off'</span>);
fh2 = matlabFunction(f,<span class="string">'vars'</span>,{x});
<span class="comment">% fh2 = objective with no gradient or Hessian</span>
[xfinal fval exitflag output2] = fminunc(fh2,[-1;2],options)
</pre><pre class="codeoutput">
Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the function tolerance.




xfinal =

   1.333337991777384
   1.037057531377053


fval =

    2.198508042259553e-011


exitflag =

     1


output2 = 

       iterations: 18
        funcCount: 81
         stepsize: 1
    firstorderopt: 2.458683006702789e-006
        algorithm: 'medium-scale: Quasi-Newton line search'
          message: [1x438 char]

</pre><p>反復回数は、勾配とヘッセ行列を使用した場合よりも少なく、関数評価の回数が激減します。</p><pre class="codeinput">sprintf([<span class="string">'There were %d iterations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.iterations,output2.iterations)
sprintf([<span class="string">'There were %d function evaluations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.funcCount,output2.funcCount)
</pre><pre class="codeoutput">
ans =

There were 14 iterations using gradient and Hessian, but 18 without them.


ans =

There were 15 function evaluations using gradient and Hessian, but 81 without them.

</pre><h2>第 2 の例: fmincon の内点法アルゴリズムを使用した制約付き最小化<a name="7"></a></h2><p>第 1 の例と同じ目的関数と初期値を検討しますが、第 2 の例には 2 つの非線形制約があります。</p><p><img src="../symbolic_optim_demo_eq26522.png" alt="$$5\sinh(x_2/5) \ge x_1^4$$"></p><p><img src="../symbolic_optim_demo_eq92336.png" alt="$$5\tanh(x_1/5) \ge x_2^2 - 1.$$"></p><p>この制約により、最適化が大域的最小点 [1.333,1.037] から離されます。この 2 つの制約を視覚化します。</p><pre class="codeinput">[X,Y] = meshgrid(-2:.01:3);
Z = (5*sinh(Y./5) &gt;= X.^4);
<span class="comment">% Z=1 where the first constraint is satisfied, Z=0 otherwise</span>
Z = Z+ 2*(5*tanh(X./5) &gt;= Y.^2 - 1);
<span class="comment">% Z=2 where the second is satisfied, Z=3 where both are</span>
surf(X,Y,Z,<span class="string">'LineStyle'</span>,<span class="string">'none'</span>);
set(gcf,<span class="string">'Color'</span>,<span class="string">'w'</span>) <span class="comment">% white background</span>
view(0,90)
hold <span class="string">on</span>
plot3(.4396, .0373, 4,<span class="string">'o'</span>,<span class="string">'MarkerEdgeColor'</span>,<span class="string">'r'</span>,<span class="string">'MarkerSize'</span>,8);
<span class="comment">% best point</span>
xlabel(<span class="string">'x'</span>);ylabel(<span class="string">'y'</span>);
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="../symbolic_optim_demo_02.png" alt=""> <p>最適点の周囲に小さい赤色の円をプロットしました。</p><p>実行可能領域全体にわたって目的関数のプロットがあります。これは両方の制約を満たす領域であり、最適点の周囲の小さい赤色の円と共に濃い赤色で示されています。</p><pre class="codeinput">W = log(1 + 3*(Y - (X.^3 - X)).^2 + (X - 4/3).^2);
<span class="comment">% W = the objective function</span>
W(Z &lt; 3) = nan; <span class="comment">% plot only where the constraints are satisfied</span>
surf(X,Y,W,<span class="string">'LineStyle'</span>,<span class="string">'none'</span>);
view(68,20)
hold <span class="string">on</span>
plot3(.4396, .0373, .8152,<span class="string">'o'</span>,<span class="string">'MarkerEdgeColor'</span>,<span class="string">'r'</span>, <span class="keyword">...</span>
    <span class="string">'MarkerSize'</span>,8); <span class="comment">% best point</span>
xlabel(<span class="string">'x'</span>);ylabel(<span class="string">'y'</span>);zlabel(<span class="string">'z'</span>);
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="../symbolic_optim_demo_03.png" alt=""> <p>非線形制約は、<tt>c(x) &lt;= 0</tt> という形式で記述しなければなりません。<tt>matlabFunction</tt> を使用して、すべてのシンボリック制約とその導関数を計算し、関数ハンドルに配置します。</p><p>これらの制約の勾配は、列ベクトルであることが望ましいです。これらは、各列が 1 つの制約関数の勾配を表すような行列として目的関数に配置しなければなりません。これは <tt>jacobian</tt> によって生成される形式の転置行列であるため、以下では転置行列を求めます。</p><p>非線形制約を関数ハンドルに配置します。<tt>fmincon</tt> は、非線形制約と勾配が [c ceq gradc gradceq] という順序で出力されると想定します。非線形等式制約が存在しないため、<tt>ceq</tt> と <tt>gradceq</tt> として [ ] を出力します。</p><pre class="codeinput">c1 = x1^4 - 5*sinh(x2/5);
c2 = x2^2 - 5*tanh(x1/5) - 1;
c = [c1 c2];
gradc = jacobian(c,x).'; <span class="comment">% transpose to put in correct form</span>
constraint = matlabFunction(c,[],gradc,[],<span class="string">'vars'</span>,{x});
</pre><p>内点法アルゴリズムでは、ヘッセ行列関数を、目的関数の一部ではなく別個の関数として記述しなければなりません。これは、非線形制約関数でこれらの制約をヘッセ行列に含める必要があるからです。内点法アルゴリズムのヘッセ行列は、ラグランジアンのヘッセ行列です。詳細は、ユーザー ガイドを参照してください。</p><p>ヘッセ行列関数には、2 つの入力引数を指定します。正のベクトル <tt>x</tt> とラグランジュ乗数構造体 lambda です。非線形制約に使用する lambda 構造体の部分は、<tt>lambda.ineqnonlin</tt> と <tt>lambda.eqnonlin</tt> です。現在の制約の場合は、非線形等式が存在しないため、<tt>lambda.ineqnonlin(1)</tt> と <tt>lambda.ineqnonlin(2)</tt> という乗数を使用します。</p><p>第 1 の例では、目的関数のヘッセ行列を計算しました。ここでは、2 つの制約関数のヘッセ行列を計算し、<tt>matlabFunction</tt> で関数ハンドル バージョンを作成します。</p><pre class="codeinput">hessc1 = jacobian(gradc(:,1),x); <span class="comment">% constraint = first c column</span>
hessc2 = jacobian(gradc(:,2),x);

hessfh = matlabFunction(hessf,<span class="string">'vars'</span>,{x});
hessc1h = matlabFunction(hessc1,<span class="string">'vars'</span>,{x});
hessc2h = matlabFunction(hessc2,<span class="string">'vars'</span>,{x});
</pre><p>最終的なヘッセ行列を作成するために、3 つのヘッセ行列を統合し、適切なラグランジュ乗数を制約関数に追加します。</p><pre class="codeinput">myhess = @(x,lambda)(hessfh(x) + <span class="keyword">...</span>
    lambda.ineqnonlin(1)*hessc1h(x) + <span class="keyword">...</span>
    lambda.ineqnonlin(2)*hessc2h(x));
</pre><p>内点法アルゴリズム、勾配、およびヘッセ行列を使用するためのオプションを設定し、目的関数が目的と勾配の両方を返すようにし、ソルバーを実行します。</p><pre class="codeinput">options = optimset(<span class="string">'Algorithm'</span>,<span class="string">'interior-point'</span>,<span class="string">'GradObj'</span>,<span class="keyword">...</span>
    <span class="string">'on'</span>,<span class="string">'GradConstr'</span>,<span class="string">'on'</span>,<span class="string">'Hessian'</span>,<span class="string">'user-supplied'</span>,<span class="keyword">...</span>
    <span class="string">'HessFcn'</span>,myhess,<span class="string">'Display'</span>,<span class="string">'final'</span>);
<span class="comment">% fh2 = objective without Hessian</span>
fh2 = matlabFunction(f,gradf,<span class="string">'vars'</span>,{x});
[xfinal fval exitflag output] = fmincon(fh2,[-1;2],<span class="keyword">...</span>
    [],[],[],[],[],[],constraint,options)
</pre><pre class="codeoutput">
Local minimum found that satisfies the constraints.

Optimization completed because the objective function is non-decreasing in 
feasible directions, to within the default value of the function tolerance,
and constraints were satisfied to within the default value of the constraint tolerance.




xfinal =

   0.439569087135554
   0.037334020327260


fval =

   0.815247080686200


exitflag =

     1


output = 

         iterations: 10
          funcCount: 13
    constrviolation: 0
           stepsize: 1.916032009774339e-006
          algorithm: 'interior-point'
      firstorderopt: 1.921701378582164e-008
       cgiterations: 0
            message: [1x782 char]

</pre><p>この例でも、勾配とヘッセ行列が与えられている場合の方がそうでない場合よりも、ソルバーによって反復と関数評価の回数が少なくなっています。</p><pre class="codeinput">options = optimset(<span class="string">'Algorithm'</span>,<span class="string">'interior-point'</span>,<span class="keyword">...</span>
    <span class="string">'Display'</span>,<span class="string">'final'</span>);
<span class="comment">% fh3 = objective without gradient or Hessian</span>
fh3 = matlabFunction(f,<span class="string">'vars'</span>,{x});
<span class="comment">% constraint without gradient:</span>
constraint = matlabFunction(c,[],<span class="string">'vars'</span>,{x});
[xfinal fval exitflag output2] = fmincon(fh3,[-1;2],<span class="keyword">...</span>
    [],[],[],[],[],[],constraint,options)

sprintf([<span class="string">'There were %d iterations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>],<span class="keyword">...</span>
    output.iterations,output2.iterations)
sprintf([<span class="string">'There were %d function evaluations using gradient'</span> <span class="keyword">...</span>
    <span class="string">' and Hessian, but %d without them.'</span>], <span class="keyword">...</span>
    output.funcCount,output2.funcCount)
</pre><pre class="codeoutput">
Local minimum found that satisfies the constraints.

Optimization completed because the objective function is non-decreasing in 
feasible directions, to within the default value of the function tolerance,
and constraints were satisfied to within the default value of the constraint tolerance.




xfinal =

   0.439568846240534
   0.037334303592384


fval =

   0.815247460667077


exitflag =

     1


output2 = 

         iterations: 17
          funcCount: 54
    constrviolation: 0
           stepsize: 4.241701937721570e-006
          algorithm: 'interior-point'
      firstorderopt: 3.843306486672748e-007
       cgiterations: 0
            message: [1x782 char]


ans =

There were 10 iterations using gradient and Hessian, but 17 without them.


ans =

There were 13 function evaluations using gradient and Hessian, but 54 without them.

</pre><h2>シンボリック変数のクリーンアップ<a name="14"></a></h2><p>このデモで使用したシンボリック変数は、実数であると想定されていました。この想定をシンボリック エンジン ワークスペースから取り除くには、これらの変数を削除するだけでは不十分です。以下の構文を使用して、変数を消去するか</p><pre class="codeinput">syms <span class="string">x1</span> <span class="string">x2</span> <span class="string">clear</span>
</pre><p>以下のコマンドを使用して、シンボリック エンジンをリセットします。</p><pre>% reset(symengine) % uncomment this line to reset the engine</pre><p>シンボリック エンジンをリセットした後、すべてのシンボリック変数を MATLAB&reg; ワークスペースから消去してください。</p><pre>% clear % uncomment this line to clear the variables</pre><p class="footer">Copyright 1990-2010 The MathWorks, Inc.<br>Published with MATLAB&reg; 7.11</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!-- ##### SOURCE BEGIN ##### %% Using Symbolic Mathematics with Optimization Toolbox(TM) Solvers % % Optimization Toolbox(TM) solvers are usually more accurate and % efficient when you supply gradients and Hessians of the % objective and constraint functions. This demo shows how to use % the Symbolic Math Toolbox(TM) functions named |jacobian| and % |matlabFunction| to provide these derivatives to optimization % solvers. % % *Additional Requirements:* % % * Symbolic Math Toolbox % % There are several considerations in using symbolic % calculations with optimization functions: % % 1. Optimization objective and constraint functions should be % defined in terms of a vector, say |x|. However, symbolic % variables are scalar or complex-valued, not vector-valued. % This requires you to translate between vectors and scalars. % % 2. Optimization gradients, and sometimes Hessians, are % supposed to be calculated within the body of the objective or % constraint functions. This means that a symbolic gradient or % Hessian has to be placed in the appropriate place in the % objective or constraint function file or function handle. % % 3. Calculating gradients and Hessians symbolically can be % time-consuming. Therefore you should perform this calculation % only once, and generate code, via |matlabFunction|, to call % during execution of the solver. % % 4. Evaluating symbolic expressions with the |subs| function is % time-consuming. It is much more efficient to use % |matlabFunction|. % % 5. |matlabFunction| generates code that depends on the % orientation of input vectors. Since |fmincon| calls the % objective function with column vectors, you must be careful to % call |matlabFunction| with column vectors of symbolic % variables.  %   Copyright 1990-2010 The MathWorks, Inc. %   $Revision: 1.1.4.12.2.1 $ $Date: 2010/07/29 21:28:57 $   %% First Example: Unconstrained Minimization with Hessian % % The objective function to minimize is: % % $$f(x_1, x_2) = \log\left (1 + 3 \left ( x_2 - (x_1^3 - x_1) % \right )^2 + (x_1 - 4/3)^2 \right ).$$ % % This function is positive, with a unique minimum value of zero % attained at |x1| = 4/3, |x2| =(4/3)^3 - 4/3 = 1.0370... % % We write the independent variables as |x1| and |x2| because in % this form they can be used as symbolic variables. As % components of a vector |x| they would be written |x(1)| and % |x(2)|. The function has a twisty valley as depicted in the % plot below.  syms x1 x2 real x = [x1;x2]; % column vector of symbolic variables f = log(1 + 3*(x2 - (x1^3 - x1))^2 + (x1 - 4/3)^2);  ezsurfc(f,[-2 2]) view(127,38)  %% % Compute the gradient and Hessian of f:  gradf = jacobian(f,x).'; % column gradf hessf = jacobian(gradf,x);  %% % The |fminunc| solver expects to pass in a vector x, and, with % the |GradObj| and |Hessian| options set to 'on', expects a % list of three outputs: [f(x),gradf(x),hessf(x)] % % |matlabFunction| generates exactly this list of three outputs % from a list of three inputs. Furthermore, using the |vars| % option, |matlabFunction| accepts vector inputs.  fh = matlabFunction(f,gradf,hessf,'vars',{x});  %% % Now solve the minimization problem starting at the point % [-1,2]:  options = optimset('GradObj','on','Hessian','on', ...     'Display','final'); [xfinal fval exitflag output] = fminunc(fh,[-1;2],options)  %% % Compare this with the number of iterations using no gradient % or Hessian information. This requires the medium-scale % algorithm, obtained by setting the |LargeScale| option to % |'off'|:  options = optimset('Display','final','LargeScale','off'); fh2 = matlabFunction(f,'vars',{x});  % fh2 = objective with no gradient or Hessian [xfinal fval exitflag output2] = fminunc(fh2,[-1;2],options)  %% % The number of iterations is lower when using gradients and % Hessians, and there are dramatically fewer function % evaluations:  sprintf(['There were %d iterations using gradient' ...     ' and Hessian, but %d without them.'], ...     output.iterations,output2.iterations) sprintf(['There were %d function evaluations using gradient' ...     ' and Hessian, but %d without them.'], ...     output.funcCount,output2.funcCount)  %% Second Example: Constrained Minimization Using the fmincon Interior-Point Algorithm % % We consider the same objective function and starting point, % but now have two nonlinear constraints: % % $$5\sinh(x_2/5) \ge x_1^4$$ % % $$5\tanh(x_1/5) \ge x_2^2 - 1.$$ % % The constraints keep the optimization away from the global % minimum point [1.333,1.037]. Visualize the two constraints:  [X,Y] = meshgrid(-2:.01:3); Z = (5*sinh(Y./5) >= X.^4);  % Z=1 where the first constraint is satisfied, Z=0 otherwise Z = Z+ 2*(5*tanh(X./5) >= Y.^2 - 1);  % Z=2 where the second is satisfied, Z=3 where both are surf(X,Y,Z,'LineStyle','none'); set(gcf,'Color','w') % white background view(0,90) hold on plot3(.4396, .0373, 4,'o','MarkerEdgeColor','r','MarkerSize',8);  % best point xlabel('x');ylabel('y'); hold off  %% % We plotted a small red circle around the optimal point. % % Here is a plot of the objective function over the feasible % region, the region that satisfies both constraints, pictured % above in dark red, along with a small red circle around the % optimal point:  W = log(1 + 3*(Y - (X.^3 - X)).^2 + (X - 4/3).^2);  % W = the objective function W(Z < 3) = nan; % plot only where the constraints are satisfied surf(X,Y,W,'LineStyle','none'); view(68,20) hold on plot3(.4396, .0373, .8152,'o','MarkerEdgeColor','r', ...     'MarkerSize',8); % best point xlabel('x');ylabel('y');zlabel('z'); hold off  %% % The nonlinear constraints must be written in the form |c(x) % <= 0|. We compute all the symbolic constraints and their % derivatives, and place them in a function handle using % |matlabFunction|. % % The gradients of the constraints should be column vectors; % they must be placed in the objective function as a matrix, % with each column of the matrix representing the gradient of % one constraint function. This is the transpose of the form % generated by |jacobian|, so we take the transpose below. % % We place the nonlinear constraints into a function handle. % |fmincon| expects the nonlinear constraints and gradients to % be output in the order [c ceq gradc gradceq]. Since there are % no nonlinear equality constraints, we output [ ] for |ceq| and % |gradceq|.  c1 = x1^4 - 5*sinh(x2/5); c2 = x2^2 - 5*tanh(x1/5) - 1; c = [c1 c2]; gradc = jacobian(c,x).'; % transpose to put in correct form constraint = matlabFunction(c,[],gradc,[],'vars',{x});  %% % The interior-point algorithm requires its Hessian function to % be written as a separate function, instead of being part of % the objective function. This is because a nonlinearly % constrained function needs to include those constraints in its % Hessian. Its Hessian is the Hessian of the Lagrangian; see the % User's Guide for more information. % % The Hessian function takes two input arguments: the position % vector |x|, and the Lagrange multiplier structure lambda. The % parts of the lambda structure that you use for nonlinear % constraints are |lambda.ineqnonlin| and |lambda.eqnonlin|. For % the current constraint, there are no linear equalities, so we % use the two multipliers |lambda.ineqnonlin(1)| and % |lambda.ineqnonlin(2)|. % % We calculated the Hessian of the objective function in the % first example. Now we calculate the Hessians of the two % constraint functions, and make function handle versions with % |matlabFunction|.  hessc1 = jacobian(gradc(:,1),x); % constraint = first c column hessc2 = jacobian(gradc(:,2),x);  hessfh = matlabFunction(hessf,'vars',{x}); hessc1h = matlabFunction(hessc1,'vars',{x}); hessc2h = matlabFunction(hessc2,'vars',{x});  %% % To make the final Hessian, we put the three Hessians together, % adding the appropriate Lagrange multipliers to the constraint % functions.  myhess = @(x,lambda)(hessfh(x) + ...     lambda.ineqnonlin(1)*hessc1h(x) + ...     lambda.ineqnonlin(2)*hessc2h(x));  %% % Set the options to use the interior-point algorithm, the % gradient, and the Hessian, have the objective function return % both the objective and the gradient, and run the solver:  options = optimset('Algorithm','interior-point','GradObj',...     'on','GradConstr','on','Hessian','user-supplied',...     'HessFcn',myhess,'Display','final'); % fh2 = objective without Hessian fh2 = matlabFunction(f,gradf,'vars',{x}); [xfinal fval exitflag output] = fmincon(fh2,[-1;2],...     [],[],[],[],[],[],constraint,options) %% % Again, the solver makes many fewer iterations and function % evaluations with gradient and Hessian supplied than when they % are not:  options = optimset('Algorithm','interior-point',...     'Display','final'); % fh3 = objective without gradient or Hessian fh3 = matlabFunction(f,'vars',{x}); % constraint without gradient: constraint = matlabFunction(c,[],'vars',{x}); [xfinal fval exitflag output2] = fmincon(fh3,[-1;2],...     [],[],[],[],[],[],constraint,options)  sprintf(['There were %d iterations using gradient' ...     ' and Hessian, but %d without them.'],...     output.iterations,output2.iterations) sprintf(['There were %d function evaluations using gradient' ...     ' and Hessian, but %d without them.'], ...     output.funcCount,output2.funcCount)  %% Cleaning Up Symbolic Variables % % The symbolic variables used in this demo were assumed to be % real. To clear this assumption from the symbolic engine % workspace, it is not sufficient to delete the variables. You % must either clear the variables using the syntax  syms x1 x2 clear  %% % or reset the symbolic engine using the command % %  % reset(symengine) % uncomment this line to reset the engine % % After resetting the symbolic engine you should clear all % symbolic variables from the MATLAB(R) workspace: % %  % clear % uncomment this line to clear the variables  displayEndOfDemoMessage(mfilename)  ##### SOURCE END ##### --></body></html>